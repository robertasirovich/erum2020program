<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.3 Regular talk | eRum2020 Program</title>
  <meta name="description" content="1.3 Regular talk | eRum2020 Program" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="1.3 Regular talk | eRum2020 Program" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.3 Regular talk | eRum2020 Program" />
  
  
  

<meta name="author" content="eRum2020 organizing Committee" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="poster.html"/>
<link rel="next" href="shiny-demo.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> eRum2020 Program Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="lightning-talk.html"><a href="lightning-talk.html"><i class="fa fa-check"></i><b>1.1</b> Lightning talk</a><ul>
<li class="chapter" data-level="1.1.1" data-path="lightning-talk.html"><a href="lightning-talk.html#an-enriched-disease-risk-assessment-model-based-on-historical-blood-donors-records"><i class="fa fa-check"></i><b>1.1.1</b> An enriched disease risk assessment model based on historical blood donors records</a></li>
<li class="chapter" data-level="1.1.2" data-path="lightning-talk.html"><a href="lightning-talk.html#automate-flexdashboard-with-github"><i class="fa fa-check"></i><b>1.1.2</b> Automate flexdashboard with GitHub</a></li>
<li class="chapter" data-level="1.1.3" data-path="lightning-talk.html"><a href="lightning-talk.html#supporting-twitter-analytics-application-with-graph-databases-and-the-arangodb-package"><i class="fa fa-check"></i><b>1.1.3</b> Supporting Twitter analytics application with graph-databases and the aRangodb package</a></li>
<li class="chapter" data-level="1.1.4" data-path="lightning-talk.html"><a href="lightning-talk.html#using-open-access-data-to-derive-genome-composition-of-emerging-viruses"><i class="fa fa-check"></i><b>1.1.4</b> Using open-access data to derive genome composition of emerging viruses</a></li>
<li class="chapter" data-level="1.1.5" data-path="lightning-talk.html"><a href="lightning-talk.html#ptmixed-an-r-package-for-flexible-modelling-of-longitudinal-overdispersed-count-data"><i class="fa fa-check"></i><b>1.1.5</b> ptmixed: an R package for flexible modelling of longitudinal overdispersed count data</a></li>
<li class="chapter" data-level="1.1.6" data-path="lightning-talk.html"><a href="lightning-talk.html#one-way-non-normal-anova-in-reliability-analysis-using-with-doex"><i class="fa fa-check"></i><b>1.1.6</b> One-way non-normal ANOVA in reliability analysis using with doex</a></li>
<li class="chapter" data-level="1.1.7" data-path="lightning-talk.html"><a href="lightning-talk.html#towards-more-structured-data-quality-assessment-in-the-process-mining-field-the-daqapo-package"><i class="fa fa-check"></i><b>1.1.7</b> Towards more structured data quality assessment in the process mining field: the DaQAPO package</a></li>
<li class="chapter" data-level="1.1.8" data-path="lightning-talk.html"><a href="lightning-talk.html#supporting-r-in-the-binder-community"><i class="fa fa-check"></i><b>1.1.8</b> Supporting R in the Binder Community</a></li>
<li class="chapter" data-level="1.1.9" data-path="lightning-talk.html"><a href="lightning-talk.html#time-series-missing-data-visualizations"><i class="fa fa-check"></i><b>1.1.9</b> Time Series Missing Data Visualizations</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="poster.html"><a href="poster.html"><i class="fa fa-check"></i><b>1.2</b> Poster</a><ul>
<li class="chapter" data-level="1.2.1" data-path="poster.html"><a href="poster.html#modified-likelihood-ratio-model-for-handwriting-recognition-in-forensic-science."><i class="fa fa-check"></i><b>1.2.1</b> Modified likelihood ratio model for handwriting recognition in forensic science.</a></li>
<li class="chapter" data-level="1.2.2" data-path="poster.html"><a href="poster.html#guidance-for-teaching-r-to-non-programmers"><i class="fa fa-check"></i><b>1.2.2</b> Guidance for teaching R to non-programmers</a></li>
<li class="chapter" data-level="1.2.3" data-path="poster.html"><a href="poster.html#how-r-hub-can-help-you-develop-and-maintain-your-r-packages"><i class="fa fa-check"></i><b>1.2.3</b> How R-hub can help you develop and maintain your R packages</a></li>
<li class="chapter" data-level="1.2.4" data-path="poster.html"><a href="poster.html#partitional-clustering-with-extensions"><i class="fa fa-check"></i><b>1.2.4</b> Partitional clustering with extensions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="regular-talk.html"><a href="regular-talk.html"><i class="fa fa-check"></i><b>1.3</b> Regular talk</a><ul>
<li class="chapter" data-level="1.3.1" data-path="regular-talk.html"><a href="regular-talk.html#the-r-package-flexreg-regression-mixture-models-for-bounded-responses"><i class="fa fa-check"></i><b>1.3.1</b> The R-package ‘FlexReg’: regression mixture models for bounded responses</a></li>
<li class="chapter" data-level="1.3.2" data-path="regular-talk.html"><a href="regular-talk.html#a-flexible-dashboard-for-monitoring-platform-trials"><i class="fa fa-check"></i><b>1.3.2</b> A flexible dashboard for monitoring platform trials</a></li>
<li class="chapter" data-level="1.3.3" data-path="regular-talk.html"><a href="regular-talk.html#using-xgboost-plumber-and-docker-in-production-to-power-a-new-banking-product"><i class="fa fa-check"></i><b>1.3.3</b> Using XGBoost, Plumber and Docker in production to power a new banking product</a></li>
<li class="chapter" data-level="1.3.4" data-path="regular-talk.html"><a href="regular-talk.html#astronomical-source-detection-and-background-separation-a-bayesian-nonparametric-approach"><i class="fa fa-check"></i><b>1.3.4</b> Astronomical source detection and background separation: a Bayesian nonparametric approach</a></li>
<li class="chapter" data-level="1.3.5" data-path="regular-talk.html"><a href="regular-talk.html#high-dimensional-sampling-and-volume-computation"><i class="fa fa-check"></i><b>1.3.5</b> High dimensional sampling and volume computation</a></li>
<li class="chapter" data-level="1.3.6" data-path="regular-talk.html"><a href="regular-talk.html#next-generation-supply-chain-planning-with-r-a-case-study"><i class="fa fa-check"></i><b>1.3.6</b> Next Generation Supply Chain Planning With R: A Case Study</a></li>
<li class="chapter" data-level="1.3.7" data-path="regular-talk.html"><a href="regular-talk.html#genetonic-enjoy-rna-seq-data-analysis-responsibly"><i class="fa fa-check"></i><b>1.3.7</b> GeneTonic: enjoy RNA-seq data analysis, responsibly</a></li>
<li class="chapter" data-level="1.3.8" data-path="regular-talk.html"><a href="regular-talk.html#a-simple-and-flexible-inactivitysleep-detection-r-package"><i class="fa fa-check"></i><b>1.3.8</b> A simple and flexible inactivity/sleep detection R package</a></li>
<li class="chapter" data-level="1.3.9" data-path="regular-talk.html"><a href="regular-talk.html#shazam-in-r-audio-analysis-using-the-av-package"><i class="fa fa-check"></i><b>1.3.9</b> Shazam in R? Audio analysis using the ‘av’ package</a></li>
<li class="chapter" data-level="1.3.10" data-path="regular-talk.html"><a href="regular-talk.html#deep-learning-and-time-series-approaches-for-improvement-of-vehicle-distribution-process"><i class="fa fa-check"></i><b>1.3.10</b> Deep learning and time series approaches for improvement of vehicle distribution process</a></li>
<li class="chapter" data-level="1.3.11" data-path="regular-talk.html"><a href="regular-talk.html#dm-working-with-relational-data-models-in-r"><i class="fa fa-check"></i><b>1.3.11</b> dm: working with relational data models in R</a></li>
<li class="chapter" data-level="1.3.12" data-path="regular-talk.html"><a href="regular-talk.html#powering-turing-e-atlas-with-r"><i class="fa fa-check"></i><b>1.3.12</b> Powering Turing e-Atlas with R</a></li>
<li class="chapter" data-level="1.3.13" data-path="regular-talk.html"><a href="regular-talk.html#an-innovative-way-to-support-your-sales-force"><i class="fa fa-check"></i><b>1.3.13</b> An innovative way to support your sales force</a></li>
<li class="chapter" data-level="1.3.14" data-path="regular-talk.html"><a href="regular-talk.html#damirseq-2.0-from-high-dimensional-data-to-cost-effective-reliable-prediction-models"><i class="fa fa-check"></i><b>1.3.14</b> DaMiRseq 2.0: from high dimensional data to cost-effective reliable prediction models</a></li>
<li class="chapter" data-level="1.3.15" data-path="regular-talk.html"><a href="regular-talk.html#interpretable-and-accessible-deep-learning-for-omics-data-with-r-and-friends"><i class="fa fa-check"></i><b>1.3.15</b> Interpretable and accessible Deep Learning for omics data with R and friends</a></li>
<li class="chapter" data-level="1.3.16" data-path="regular-talk.html"><a href="regular-talk.html#analyzing-preference-data-with-the-bayesmallows-package"><i class="fa fa-check"></i><b>1.3.16</b> Analyzing Preference Data with the BayesMallows Package</a></li>
<li class="chapter" data-level="1.3.17" data-path="regular-talk.html"><a href="regular-talk.html#analyzing-preference-data-with-the-bayesmallows-package-1"><i class="fa fa-check"></i><b>1.3.17</b> Analyzing Preference Data with the BayesMallows Package</a></li>
<li class="chapter" data-level="1.3.18" data-path="regular-talk.html"><a href="regular-talk.html#flexible-meta-analysis-of-generalized-additive-models-with-metagam"><i class="fa fa-check"></i><b>1.3.18</b> Flexible Meta-Analysis of Generalized Additive Models with metagam</a></li>
<li class="chapter" data-level="1.3.19" data-path="regular-talk.html"><a href="regular-talk.html#flexible-meta-analysis-of-generalized-additive-models-with-metagam-1"><i class="fa fa-check"></i><b>1.3.19</b> Flexible Meta-Analysis of Generalized Additive Models with metagam</a></li>
<li class="chapter" data-level="1.3.20" data-path="regular-talk.html"><a href="regular-talk.html#controlled-r-development-with-docker"><i class="fa fa-check"></i><b>1.3.20</b> Controlled R development with Docker</a></li>
<li class="chapter" data-level="1.3.21" data-path="regular-talk.html"><a href="regular-talk.html#interactive-visualization-of-complex-texts"><i class="fa fa-check"></i><b>1.3.21</b> Interactive visualization of complex texts</a></li>
<li class="chapter" data-level="1.3.22" data-path="regular-talk.html"><a href="regular-talk.html#connector-a-computational-approach-to-study-intratumor-heterogeneity."><i class="fa fa-check"></i><b>1.3.22</b> CONNECTOR: a computational approach to study intratumor heterogeneity.</a></li>
<li class="chapter" data-level="1.3.23" data-path="regular-talk.html"><a href="regular-talk.html#flexible-deep-learning-via-the-juliaconnector"><i class="fa fa-check"></i><b>1.3.23</b> Flexible deep learning via the JuliaConnectoR</a></li>
<li class="chapter" data-level="1.3.24" data-path="regular-talk.html"><a href="regular-talk.html#gwqs-an-r-package-for-linear-and-generalized-weighted-quantile-sum-wqs-regression"><i class="fa fa-check"></i><b>1.3.24</b> gWQS: An R Package for Linear and Generalized Weighted Quantile Sum (WQS) Regression</a></li>
<li class="chapter" data-level="1.3.25" data-path="regular-talk.html"><a href="regular-talk.html#rlinkedcharts-a-novel-approach-for-simple-but-powerful-interactive-data-analysis"><i class="fa fa-check"></i><b>1.3.25</b> R/LinkedCharts: A novel approach for simple but powerful interactive data analysis</a></li>
<li class="chapter" data-level="1.3.26" data-path="regular-talk.html"><a href="regular-talk.html#transparent-journalism-through-the-power-of-r"><i class="fa fa-check"></i><b>1.3.26</b> Transparent Journalism Through the Power of R</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="shiny-demo.html"><a href="shiny-demo.html"><i class="fa fa-check"></i><b>1.4</b> Shiny demo</a><ul>
<li class="chapter" data-level="1.4.1" data-path="shiny-demo.html"><a href="shiny-demo.html#decision-support-for-maritime-spatial-planning"><i class="fa fa-check"></i><b>1.4.1</b> Decision support for maritime spatial planning</a></li>
<li class="chapter" data-level="1.4.2" data-path="shiny-demo.html"><a href="shiny-demo.html#automated-receptive-and-interactive-a-classroom-based-data-generation-exercise-using-shiny"><i class="fa fa-check"></i><b>1.4.2</b> Automated, receptive and interactive: a classroom-based data generation exercise using Shiny</a></li>
<li class="chapter" data-level="1.4.3" data-path="shiny-demo.html"><a href="shiny-demo.html#mobility-scan"><i class="fa fa-check"></i><b>1.4.3</b> Mobility scan</a></li>
<li class="chapter" data-level="1.4.4" data-path="shiny-demo.html"><a href="shiny-demo.html#scoring-the-implicit-association-test-has-never-been-easier-dscoreapp"><i class="fa fa-check"></i><b>1.4.4</b> Scoring the Implicit Association Test has never been easier: DscoreApp</a></li>
<li class="chapter" data-level="1.4.5" data-path="shiny-demo.html"><a href="shiny-demo.html#rtrhexng-hexagon-sticker-app-for-rtrng"><i class="fa fa-check"></i><b>1.4.5</b> rTRhexNG: Hexagon sticker app for rTRNG</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">eRum2020 Program</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regular-talk" class="section level2">
<h2><span class="header-section-number">1.3</span> Regular talk</h2>
<div id="the-r-package-flexreg-regression-mixture-models-for-bounded-responses" class="section level3">
<h3><span class="header-section-number">1.3.1</span> The R-package ‘FlexReg’: regression mixture models for bounded responses</h3>
<p><em>Agnese Maria Di Brisco, Postdoctoral researcher</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>The analysis of data defined on bounded intervals (such as proportions or rates) is challenging since classical linear regression models are unsuitable. A fruitful alternative to data transformation is the definition of a regression model based on distributions with proper support, like the beta, the flexible beta (FB) and the variance inflated beta (VIB), where the latter two are mixtures of betas showing a good fit even in the presence of multimodality, heavy tails and/or outliers.
This talk illustrates the FlexReg package, which allows to fit Beta, FB, and VIB regression models. The core function of the package is ‘flexreg’, which performs a Bayesian estimation via Hamiltonian Monte Carlo (HMC) algorithm through rstan package. Among the many arguments of the function, there are the model (Beta, FB or VIB) and the link functions for the mean and for the precision parameters. Also, the function allows specifying several prior distributions, the hyperparameters, and many settings of the HMC algorithm such as the number of iterations and of chains.
The FlexReg package includes several functions to compute fitting criteria, posterior predictive distributions and residuals. At last, functions that provide simple and clear plots of the regression curves, posterior predictive and residuals are available. All the features of the FlexReg package are illustrated through data from the literature.</p>
</div>
<div id="a-flexible-dashboard-for-monitoring-platform-trials" class="section level3">
<h3><span class="header-section-number">1.3.2</span> A flexible dashboard for monitoring platform trials</h3>
<p><em>Alessio Crippa, Karolinska Institutet, postdoc</em></p>
<p><strong>Track(s):</strong> R Applications</p>
<p><strong>Abstract:</strong></p>
<p>The Data and Safety Monitoring Board (DSMB) is an essential component for a successful clinical trial. It consists of an independent group of experts that periodically revise and evaluate the accumulating data from an ongoing trial to assess patients’ safety, study progress, and drug efficacy. Based on their evaluation, a recommendation to continue, modify or stop the trial will be delivered to the trial’s sponsor. It is essential to provide the DSMB with the best delivery visualization tools for monitoring on a regular basis the live data from the study trial.
We designed and developed an interactive dashboard using flexdashboard for R as a helping tool for assisting the DSMB in the evaluation of the results of the ProBio study, a clinical platform for improving treatment decision in patients with metastatic castrate resistant prostate cancer. We will focus on the customized structure for best displaying the most interesting variables and the adoption of interactive tools as a particularly useful aid for the assessment of the ongoing data. We will also cover the connection to the data sources, the automatic generation process, and the selected permission for the people in the DSMB to access the dashboard.</p>
</div>
<div id="using-xgboost-plumber-and-docker-in-production-to-power-a-new-banking-product" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Using XGBoost, Plumber and Docker in production to power a new banking product</h3>
<p><em>André Rivenæs, Data Scientist, PwC</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models, R Production</p>
<p><strong>Abstract:</strong></p>
<p>Buffer is a brand new and innovative banking product by one of the largest retail banks in Norway, Sparebanken Vest, and it is powered by R.</p>
<p>In fact, the product’s decision engine is written entirely in R. We analyze whether a customer should get a loan and how much loan they should be allocated by analyzing large amounts of data from various sources. An essential part is analyzing the customer’s invoices using machine learning (XGBoost).</p>
<p>In this talk, we will cover:</p>
<ul>
<li>How we use ML and Bayesian statistics to estimate the probability of an invoice being repaid.</li>
<li>How we successfully put the decision engine in production, using e.g. Plumber, Docker, CircleCI and Kubernetes.</li>
<li>What we have learned from using R in production at scale.</li>
</ul>
</div>
<div id="astronomical-source-detection-and-background-separation-a-bayesian-nonparametric-approach" class="section level3">
<h3><span class="header-section-number">1.3.4</span> Astronomical source detection and background separation: a Bayesian nonparametric approach</h3>
<p><em>Andrea Sottosanti, University of Padova</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models, R Applications</p>
<p><strong>Abstract:</strong></p>
<p>We propose an innovative approach based on Bayesian nonparametric methods to the signal extraction of astronomical sources in gamma-ray count maps under the presence of a strong background contamination. Our model simultaneously induces clustering on the photons using their spatial information and gives an estimate of the number of sources, while separating them from the irregular signal of the background component that extends over the entire map. From a statistical perspective, the signal of the sources is modeled using a Dirichlet Process mixture, that allows to discover and locate a possible infinite number of clusters, while the background component is completely reconstructed using a new flexible Bayesian nonparametric model based on b-spline basis functions. The resultant can be then thought of as a hierarchical mixture of nonparametric mixtures for flexible clustering of highly contaminated signals. We provide also a Markov chain Monte Carlo algorithm to infer on the posterior distribution of the model parameters, and a suitable post-processing algorithm to quantify the information coming from the detected clusters. Results on different datasets confirm the capacity of the model to discover and locate the sources in the analysed map, to quantify their intensities and to estimate and account for the presence of the background contamination.</p>
</div>
<div id="high-dimensional-sampling-and-volume-computation" class="section level3">
<h3><span class="header-section-number">1.3.5</span> High dimensional sampling and volume computation</h3>
<p><em>Apostolos Chalkis, PhD in Computer Science</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>Sampling from multivariate distributions is a fundamental problem in statistics that plays important role in modern machine learning and data science. Many important problems such as convex optimization and multivariate integration can be efficiently solved via sampling. This talk presents the CRAN package volesti which offers to R community efficient C++ implementations of state-of-the-art algorithms for sampling and volume computation of convex sets. It scales up to hundred or thousand dimensions, depending the problem, providing the most efficient implementations for sampling and volume computation to date. Thus, volesti allows users to solve problems in dimensions and order of magnitude higher than before. We present the basic functionality of volesti and show how it can be used to provide approximate solutions to intractable problems in combinatorics, financial modeling, bioinformatics and engineering. We stand out two famous applications in finance. We show how volesti can be used to detect financial crises and evaluate portfolios performance in large stock markets with hundreds of assets, by giving real life examples using public data.</p>
</div>
<div id="next-generation-supply-chain-planning-with-r-a-case-study" class="section level3">
<h3><span class="header-section-number">1.3.6</span> Next Generation Supply Chain Planning With R: A Case Study</h3>
<p><em>Benedikt Heller, Data Strategy Consultant at Continental</em></p>
<p><strong>Track(s):</strong> R Production, R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>Demand forecasting is an integral part of successful Supply Chain Planning — good estimates of future demands allow businesses to produce and store their goods efficiently, which saves resources, facilitates growth, and keeps customers happy. Recent developments in forecasting made it possible to push the envelope on demand forecasting. In 2018, Makridakis and colleagues launched the 6-month-long M4 Competition, a call to researchers and enthusiasts to predict a set of time series, which provided new insights and — thanks to publicly available source code — paved the way for others to test successful algorithms in different domains. In this talk, we will present a case study of implementing an M4-inspired forecasting solution at the German automotive manufacturing company Continental, which successfully improved forecasting accuracy. The talk covers the significant findings of the M4 Competition, presents our implementation, and summarizes project insights.</p>
</div>
<div id="genetonic-enjoy-rna-seq-data-analysis-responsibly" class="section level3">
<h3><span class="header-section-number">1.3.7</span> GeneTonic: enjoy RNA-seq data analysis, responsibly</h3>
<p><em>Federico Marini, Center for Thrombosis and Hemostasis (CTH) &amp; Institute of Medical Biostatistics, Epidemiology and Informatics (IMBEI) - University Medical Center Mainz</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Interpreting the results from RNA-seq transcriptome experiments can be a complex task, where the essential information is distributed among different tabular and list formats - normalized expression values, results from differential expression analysis, and results from functional enrichment analyses.</p>
<p>The identification of relevant functional patterns, as well as their contextualization in the data and results at hand, are not straightforward operations if these pieces of information are not combined together efficiently.</p>
<p>Interactivity can play an essential role in simplifying the way how one accesses and digests RNA-seq data analysis in a more comprehensive way.</p>
<p>I introduce <code>GeneTonic</code> (<a href="https://github.com/federicomarini/GeneTonic" class="uri">https://github.com/federicomarini/GeneTonic</a>), an application developed in Shiny and based on many essential elements of the Bioconductor project, that aims to reduce the barrier to understanding such data better, and to efficiently combine the different components of the analytic workflow.</p>
<p>For example, starting from bird’s eye perspective summaries (with interactive bipartite gene-geneset graphs, or enrichment maps), it is easy to generate a number of visualizations, where drill-down user actions enable further insight and deliver additional information (e.g., gene info boxes, geneset summary, and signature heatmaps).</p>
<p>Complex datasets interpretation can be wrapped up into a single call to the GeneTonic main function, which also supports built-in RMarkdown reporting, to both conclude an exploration session, or also to generate in batch the output of the available functionality, delivering an essential foundation for computational reproducibility.</p>
</div>
<div id="a-simple-and-flexible-inactivitysleep-detection-r-package" class="section level3">
<h3><span class="header-section-number">1.3.8</span> A simple and flexible inactivity/sleep detection R package</h3>
<p><em>Francesca Giorgolo, Kode s.r.l. - Data Scientist</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>With the widespread usage of wearable devices great amount of data became available and new fields of application arised, like health monitoring and activity detection.
Our work focused on inactivity and sleep detection from continuous raw tri-axis accelerometer data, recorded using different accelerometers brands having sampling frequencies below and above 1Hz.
The algorithm implemented is the SPT-window detection algorithm described in literature slighty modified to met the flexibility requirement we imposed ourselves.
The R package developed provides functions to clean data, to identify inactivity/sleep windows and to visualize the results.
The main function has a parameter to specify the measurement unit of the data, a threshold to distinguish low and high activity and also a parameter to handle non-wearing periods, where a non wear period is defined as a period of time where all the accelerometers are equal to zero. Other functions allow to separate overlapped accelerometer signals, i.e. when a device is replaced by another, and to visualize the obtained results.</p>
</div>
<div id="shazam-in-r-audio-analysis-using-the-av-package" class="section level3">
<h3><span class="header-section-number">1.3.9</span> Shazam in R? Audio analysis using the ‘av’ package</h3>
<p><em>Jeroen Ooms, rOpenSci, UC Berkeley</em></p>
<p><strong>Track(s):</strong> R Applications</p>
<p><strong>Abstract:</strong></p>
<p>Shazam [1] is a popular smartphone app that can quickly identify a song or movie from a short audio recording. The basic recognition algorithm [2] effectively combines a few statistical methods to fingerprint an audio fragment based on density peaks in the time-frequency graph (spectrogram). These fingerprints are robust against noise and distortion, and can quickly be compared against a database of known songs. Similar approaches are used in audio signal classification to analyze anything from human speech to whale mating calls.</p>
<p>This talk describes how we would implement something like this in R. We use the new rOpenSci package ‘av’ to read high quality audio/video files (mkv, mp3, aac, etc) into frequency data [3]. The av package makes it easy to cut, convert, and downsample audio, and customize FFT parameters, to prepare audio for analysis in R. We can visually inspect the frequency data by plotting the spectrogram, and finally try to calculate some of the spectrogram fingerprint statistics as described by the Shazam paper.</p>
<p>[1] <a href="https://www.shazam.com" class="uri">https://www.shazam.com</a>
[2] <a href="https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf" class="uri">https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf</a>
[3] <a href="https://docs.ropensci.org/av/articles/articles/spectrograms.html" class="uri">https://docs.ropensci.org/av/articles/articles/spectrograms.html</a></p>
</div>
<div id="deep-learning-and-time-series-approaches-for-improvement-of-vehicle-distribution-process" class="section level3">
<h3><span class="header-section-number">1.3.10</span> Deep learning and time series approaches for improvement of vehicle distribution process</h3>
<p><em>Julia Fumbarev, BMW Group, Data scientist</em></p>
<p><strong>Track(s):</strong> R Applications, R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>Accurate planning of transport- and stock duration is a ubiquitous problem in vehicle distribution. As BMW Group faces the annual delivery volumes of 2.7 million vehicles, the importance of process optimization through scaling up to such high volumes becomes self-evident. Precise forecasts not only ensure efficient resource allocation, but can play a crucial role in process delay resolution: If we are aware of a delay in the delivery of a vehicle to a customer at an early stage, high short-term costs can be avoided by taking corresponding measures e.g. acceleration of the follow-up process, early customer’s delay notice etc.
To improve predictions regarding the process times and arrival events, we relied on state-of-the-art approaches of deep learning and time series analysis. The methodology was applied to two use cases: predictions of inflows into ports and event date predictions. For the former, we were able to achieve more efficient utilization of the ships while for the latter, a more reliable prognosis ensures higher customer satisfaction which is the focus of BMW.
The algorithm’s performance was evaluated by computing prediction accuracy for the respective use case. The main contribution of our work is delivering a more reliable forecast that has a positive impact on the entire supply chain and, consequently, can generate large business value.</p>
</div>
<div id="dm-working-with-relational-data-models-in-r" class="section level3">
<h3><span class="header-section-number">1.3.11</span> dm: working with relational data models in R</h3>
<p><em>Kirill Müller, Clean code, tidy data. Consulting for cynkra, coding in the open.</em></p>
<p><strong>Track(s):</strong> R Applications, R Production, R World</p>
<p><strong>Abstract:</strong></p>
<p>Storing all data related to a problem in a single table or data frame (“the dataset”) can result in many repetitive values. Separation into multiple tables helps data quality but requires “merge” or “join” operations. {dm} is a new package that fills a gap in the R ecosystem: it makes working with multiple tables just as easy as working with a single table.</p>
<p>A “data model” consists of tables (both the definition and the data), and primary and foreign keys. The {dm} package combines these concepts with data manipulation powered by the tidyverse: entire data models are handled in a single entity, a “dm” object.</p>
<p>Three principal use cases for {dm} can be identified:</p>
<ol style="list-style-type: decimal">
<li><p>When you consume a data model, {dm} helps access and manipulate a dataset consisting of multiple tables (database or local data frames) through a consistent interface.</p></li>
<li><p>When you use a third-party dataset, {dm} helps normalizing the data to remove redundancies as part of the cleaning process.</p></li>
<li><p>To create a relational data model, you can prepare the data using R and familiar tools and seamlessly export to a database.</p></li>
</ol>
<p>The presentation revolves around these use cases and shows a few applications. The {dm} package is available on GitHub and will be submitted to CRAN in early February.</p>
</div>
<div id="powering-turing-e-atlas-with-r" class="section level3">
<h3><span class="header-section-number">1.3.12</span> Powering Turing e-Atlas with R</h3>
<p><em>Layik Hama, Putting R and React together</em></p>
<p><strong>Track(s):</strong> R Applications, R Production, R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>Turing e-Atlas is a research project under the Urban Analytics research theme at Alan Turing Institute (ATI). The ATI is UK’s national institute for data science and Artificial Intelligence based at the British Library in London.</p>
<p>The research is a grand vision for which we have been trying to take baby steps under the banner of an e-Atlas. And we believe R is positioned to play a foundation role in any scalable solution to analyse and visualize large scale datasets especially geospatial datasets.</p>
<p>The application presented is built using RStudio’s Plumber package which relies on solid libraries to develop web applications. The front-end is made up of Uber’s various visualization packages using Facebook’s React JavaScript framework.</p>
</div>
<div id="an-innovative-way-to-support-your-sales-force" class="section level3">
<h3><span class="header-section-number">1.3.13</span> An innovative way to support your sales force</h3>
<p><em>Matilde Grecchi, Head of Data Science &amp; Innovation <span class="citation">@ZucchettiSpa</span></em></p>
<p><strong>Track(s):</strong> R Production, R Dataviz &amp; Shiny, R Machine Learning &amp; Models, R Applications</p>
<p><strong>Abstract:</strong></p>
<p>Explanation of the web application realized in Shiny and deployed in production to support the sales force of Zucchetti. An overview of the overall step followed from data ingestion to modeling, from validation of the model to shiny web-app realization, from deployment in production to continous learning thanks to feedbacks coming from sales force and redemption of customers. All the code is written in R using RStudio. The deployment of the app is done with ShinyProxy.io</p>
</div>
<div id="damirseq-2.0-from-high-dimensional-data-to-cost-effective-reliable-prediction-models" class="section level3">
<h3><span class="header-section-number">1.3.14</span> DaMiRseq 2.0: from high dimensional data to cost-effective reliable prediction models</h3>
<p><em>Mattia Chiesa, Senior data scientist @ Centro Cardiologico Monzino IRCCS</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>High dimensional data generated by modern high-throughput platforms pose a great challenge in selecting a small number of informative variables, for biomarker discovery and classification. Machine learning is an appropriate approach to derive general knowledge from data, identifying highly discriminative features and building accurate prediction models. To this end, we developed the R/Bioconductor package DaMiRseq, which (i) helps researchers to filter and normalize high dimensional datasets, arising from RNA-Seq experiments, by removing noise and bias and (ii) exploits a custom machine learning workflow to select the minimum set of robust informative features able to discriminate classes.
Here, we present the version 2.0 of the DaMiRseq package, an extension that provides a flexible and convenient framework for managing high dimensional data such as omics data, large-scale medical histories, or even social media and financial data. Specifically, DaMiRseq 2.0 implements new functions that allow training and testing of several different classifiers and selection of the most reliable one, in terms of classification performance and number of selected features. The resulting classification model can be further used for any prediction purpose. This framework will give users the ability to build an efficient prediction model that can be easily replicated in further related settings.</p>
</div>
<div id="interpretable-and-accessible-deep-learning-for-omics-data-with-r-and-friends" class="section level3">
<h3><span class="header-section-number">1.3.15</span> Interpretable and accessible Deep Learning for omics data with R and friends</h3>
<p><em>Moritz Hess, Research Associate, Institute of Medical Biometry and Statistics, Faculty of Medicine and Medical Center - University of Freiburg</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Recently, generative Deep Learning approaches were shown to have a huge potential for e.g. retrieving compact, latent representations of high-dimensional omics data such as single-cell RNA-Seq data. However, there are no established methods to infer how these latent representations relate to the observed variables, i.e. the genes.</p>
<p>For extracting interpretable patterns from gene expression data that indicate distinct sub-populations in the data, we here employ log-linear models, applied to the synthetic data and corresponding latent representations, sampled from generative deep models, which were trained with single-cell gene expression data.</p>
<p>While omics data are routinely analyzed in R and powerful toolboxes, tailored to omics data are available, there are no established and truely accessible approaches for Deep Learning applications here.</p>
<p>To close this gap, we here demonstrate how easily customizable Deep Learning frameworks, developed for the Julia programming language, can be leveraged in R, to perform accessible and interpretable Deep Learning with omics data.</p>
</div>
<div id="analyzing-preference-data-with-the-bayesmallows-package" class="section level3">
<h3><span class="header-section-number">1.3.16</span> Analyzing Preference Data with the BayesMallows Package</h3>
<p><em>Øystein Sørensen, Associate Professor, University of Oslo</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>BayesMallows is an R package for analyzing preference data in the form of rankings with the Mallows rank model, and its finite mixture extension, in a Bayesian framework. The model is grounded on the idea that the probability density of an observed ranking decreases exponentially with the distance to the location parameter. It is the first Bayesian implementation that allows wide choices of distances, and it works well with a large number of items to be ranked. BayesMallows handles non-standard data: partial rankings and pairwise comparisons, even in cases including non-transitive preference patterns. The Bayesian paradigm allows coherent quantification of posterior uncertainties of estimates of any quantity of interest. These posteriors are fully available to the user, and the package comes with convenient tools for summarizing and visualizing the posterior distributions.</p>
<p>This talk will focus on how the BayesMallows package can be used to analyze preference data, in particular how the Bayesian paradigm allows endless possibilities in answering questions of interest with the help of visualization of posterior distributions. Such posterior summaries can easily be communicated with scientific collaborators and business stakeholders who may not be machine learning experts themselves.</p>
</div>
<div id="analyzing-preference-data-with-the-bayesmallows-package-1" class="section level3">
<h3><span class="header-section-number">1.3.17</span> Analyzing Preference Data with the BayesMallows Package</h3>
<p><em>Øystein Sørensen, Associate Professor, University of Oslo</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>BayesMallows is an R package for analyzing preference data in the form of rankings with the Mallows rank model, and its finite mixture extension, in a Bayesian framework. The model is grounded on the idea that the probability density of an observed ranking decreases exponentially with the distance to the location parameter. It is the first Bayesian implementation that allows wide choices of distances, and it works well with a large number of items to be ranked. BayesMallows handles non-standard data: partial rankings and pairwise comparisons, even in cases including non-transitive preference patterns. The Bayesian paradigm allows coherent quantification of posterior uncertainties of estimates of any quantity of interest. These posteriors are fully available to the user, and the package comes with convenient tools for summarizing and visualizing the posterior distributions.</p>
<p>This talk will focus on how the BayesMallows package can be used to analyze preference data, in particular how the Bayesian paradigm allows endless possibilities in answering questions of interest with the help of visualization of posterior distributions. Such posterior summaries can easily be communicated with scientific collaborators and business stakeholders who may not be machine learning experts themselves.</p>
</div>
<div id="flexible-meta-analysis-of-generalized-additive-models-with-metagam" class="section level3">
<h3><span class="header-section-number">1.3.18</span> Flexible Meta-Analysis of Generalized Additive Models with metagam</h3>
<p><em>Øystein Sørensen, Associate Professor, University of Oslo</em></p>
<p><strong>Track(s):</strong> R Life Sciences, R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>Analyzing biomedical data from multiple studies has great potential in terms of increasing statistical power, enabling detection of associations of smaller magnitude than would be possible analyzing each study separately. Restrictions due to privacy or proprietary data as well as more practical concerns can make it hard to share datasets, such that analyzing all data in a single mega-analysis might not be possible. Meta-analytic methods provide a way to overcome this issue, by combining aggregated quantities like model parameters or risk ratios. However, most meta-analytic tools have focused on parametric statistical models, and software for meta-analyzing semi-parametric models like generalized additive models (GAMs) have not been developed. The metagam package attempts to fill this gap: It provides functionality for removing individual participant data from GAM objects such that they can be analyzed in a common location; furthermore metagam enables meta-analysis of the resulting GAM objects, as well as various tools for visualization and statistical analysis. This talk will illustrate use of the metagam package for analysis of the relationship between sleep quality and brain structure using data from six European brain imaging cohorts.</p>
</div>
<div id="flexible-meta-analysis-of-generalized-additive-models-with-metagam-1" class="section level3">
<h3><span class="header-section-number">1.3.19</span> Flexible Meta-Analysis of Generalized Additive Models with metagam</h3>
<p><em>Øystein Sørensen, Associate Professor, University of Oslo</em></p>
<p><strong>Track(s):</strong> R Life Sciences, R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>Analyzing biomedical data from multiple studies has great potential in terms of increasing statistical power, enabling detection of associations of smaller magnitude than would be possible analyzing each study separately. Restrictions due to privacy or proprietary data as well as more practical concerns can make it hard to share datasets, such that analyzing all data in a single mega-analysis might not be possible. Meta-analytic methods provide a way to overcome this issue, by combining aggregated quantities like model parameters or risk ratios. However, most meta-analytic tools have focused on parametric statistical models, and software for meta-analyzing semi-parametric models like generalized additive models (GAMs) have not been developed. The metagam package attempts to fill this gap: It provides functionality for removing individual participant data from GAM objects such that they can be analyzed in a common location; furthermore metagam enables meta-analysis of the resulting GAM objects, as well as various tools for visualization and statistical analysis. This talk will illustrate use of the metagam package for analysis of the relationship between sleep quality and brain structure using data from six European brain imaging cohorts.</p>
</div>
<div id="controlled-r-development-with-docker" class="section level3">
<h3><span class="header-section-number">1.3.20</span> Controlled R development with Docker</h3>
<p><em>Peter Schmid, R programmer at Mirai Solutions</em></p>
<p><strong>Track(s):</strong> R Production</p>
<p><strong>Abstract:</strong></p>
<p>When deploying productive solutions, it is essential to have full control over the code-base and environment to ensure reproducibility and stability of the setup. Additionally, guaranteeing full equivalence of the development setup to (alternative) target productive stages is a key aspect of well-managed release pipelines.
In the case of R-based projects, this implies fixing and aligning the version of R as well as package and system dependencies. This is, however, often disregarded due to the absence of out-of-the-box solutions, especially in free open source projects.
With this talk we illustrate an approach to manage a version-stable R development environment that, in the context of containerized solutions based on Docker and the Rocker project, allows to regulate your setup, along with laying out a path for trouble-free deployments and releases. It additionally enables the coexistence of multiple dockerized development flavors, to match various target production environments or projects.</p>
<p>Since the approach does not rely on commercial tools, it is particularly apt for open source projects, as we showcase with the concrete example of OasisUI (<a href="https://github.com/OasisLMF/OasisUI" class="uri">https://github.com/OasisLMF/OasisUI</a>): a web-based Shiny app providing a front-end interface to the Oasis LMF platform, an open source natural catastrophe loss modelling framework, freely available at <a href="https://github.com/OasisLMF" class="uri">https://github.com/OasisLMF</a>.</p>
</div>
<div id="interactive-visualization-of-complex-texts" class="section level3">
<h3><span class="header-section-number">1.3.21</span> Interactive visualization of complex texts</h3>
<p><em>Renate Delucchi Danhier, Post-Doc, TU Dortmund</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>Hundreds of speakers may describe the same circumstance - e.g. explaining a fixed route to a goal - without producing two identical texts. The enormous variability of language and the complexity involved in encoding meaning poses a real difficulty for linguists analyzing text databases. In order to aid linguists in identifying patterns to perform comparative research, we developed an interactive shiny app that enables quantitative analysis of text corpora without oversimplifying the structure of language. Route directions are an example of complex texts, in which speakers take cognitive decisions such as segmenting the route, selecting landmarks and organizing spatial concepts into sentences. In the data visualization, symbols and colors representing linguistic concepts are plotted into coordinates that relate the information to fixed points along the route. Six interconnected layers of meaning represent the multi-layered form-to-meaning mapping characteristic of language. The shiny app allows to select and deselect information on these different layers, offering a holistic linguistic analysis way beyond the complexity attempted within traditional linguistics. The result is a kind of visual language in itself that deconstructs the interconnected layers of meaning found in natural language.</p>
</div>
<div id="connector-a-computational-approach-to-study-intratumor-heterogeneity." class="section level3">
<h3><span class="header-section-number">1.3.22</span> CONNECTOR: a computational approach to study intratumor heterogeneity.</h3>
<p><em>Simone Pernice, Ph.D.</em></p>
<p><strong>Track(s):</strong> R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Literature is characterized by a broad class of mathematical models which can be used for fitting cancer growth time series, but with no a global consensus or biological evidence that can drive the choice of the correct model. The conventional perception is that the mechanistic models enable the biological understanding of the systems under study. However, there is no way that these models can capture the variability characterizing the cancer progression, especially because of the irregularity and sparsity of the available data.
For this reason, we propose CONNECTOR, an R package built on the model-based approach for clustering functional data. Such method is based on the clustering and fitting of the data through a combination of cubic natural splines basis with coefficients treated as random variables. Our approach is particularly effective when the observations are sparse and irregularly spaced, as growth curves usually are. CONNECTOR provides a tool set to easily guide through the parameters selection, i.e., (i) the dimension of the spline basis, (ii) the dimension of the mean space and (iii) the number of clusters to fit, to be properly chosen before fitting. The effectiveness of CONNECTOR is evaluated on growth curves of Patient Derived Xenografts (PDXs) of ovarian cancer. Genomic analyses of PDXs allowed correlating fitted and clustered PDX growth curves to cell population distribution.</p>
</div>
<div id="flexible-deep-learning-via-the-juliaconnector" class="section level3">
<h3><span class="header-section-number">1.3.23</span> Flexible deep learning via the JuliaConnectoR</h3>
<p><em>Stefan Lenz, Statistician at the Institute of Medical Biometry and Statistics (IMBI), Faculty of Medicine and Medical Center – University of Freiburg</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models</p>
<p><strong>Abstract:</strong></p>
<p>For deep learning in R, frameworks from other languages, e. g. from Python, are widely used. Julia is another language which offers computational speed and a growing ecosystem for machine learning, e. g. with the package “Flux”. Integrating functionality of Julia in R is especially promising due to the many commonalities of Julia and R. We take advantage of these in the design of our “JuliaConnectoR” R package, which aims at a tight integration of Julia in R. We would like to present our package, together with some deep learning examples.
The JuliaConnectoR can import Julia functions, also from whole packages, and make them directly callable in R. Values and data structures are translated between the two languages. This includes the management of objects holding external resources such as memory pointers. The possibility to pass R functions as arguments to Julia functions makes the JuliaConnectoR a truly functional interface. Such callback functions can, e. g., be used to interactively display the learning process of a neural network in R while it is trained in Julia. Among others, this feature sets the JuliaConnectoR apart from the other R packages for integrating Julia in R, “XRJulia” and “JuliaCall”. This becomes possible with an optimized communication protocol, which also allows a highly efficient data transfer, leveraging the similarities in the binary representation of values in Julia and R.</p>
</div>
<div id="gwqs-an-r-package-for-linear-and-generalized-weighted-quantile-sum-wqs-regression" class="section level3">
<h3><span class="header-section-number">1.3.24</span> gWQS: An R Package for Linear and Generalized Weighted Quantile Sum (WQS) Regression</h3>
<p><em>Stefano Renzetti, PhD Student at Università degli Studi di Milano</em></p>
<p><strong>Track(s):</strong> R Machine Learning &amp; Models, R Life Sciences</p>
<p><strong>Abstract:</strong></p>
<p>Weighted Quantile Sum (WQS) regression is a statistical model for multivariate regression in high-dimensional datasets commonly encountered in environmental exposures. The model constructs a weighted index estimating the mixture effect associated with all predictor variables on an outcome. The package gWQS extends WQS regression to applications with continuous, categorical and count outcomes. We provide four examples to illustrate the usage of the package.</p>
</div>
<div id="rlinkedcharts-a-novel-approach-for-simple-but-powerful-interactive-data-analysis" class="section level3">
<h3><span class="header-section-number">1.3.25</span> R/LinkedCharts: A novel approach for simple but powerful interactive data analysis</h3>
<p><em>Svetlana Ovchinnikova, Doctoral student, Zentrum für Molekulare Biologie der Universität Heidelberg</em></p>
<p><strong>Track(s):</strong> R Dataviz &amp; Shiny</p>
<p><strong>Abstract:</strong></p>
<p>In exploratory data analysis, one usually jumps back and forth between visualizations that provide overview of the whole data and others that dive into details. In data quality assessment, for example, it might be very helpful to have one chart showing a summary statistic for all samples, and clicking on one of the data points would display details on this sample in a second plot. Setting up such interactively linked charts is usually cumbersome and time-consuming to use them in ad hoc analysis. We present R/LinkedChart, a framework that renders this tasks radically simple: Producing linked charts is as quickly done as is producing conventional static plots in R, requiring a data scientist to write only very few lines of simple R code to obtain complex and general visualization. We expect that the convinience of our new tool will enable data scientists and bioinformaticians to perform much deeper and more thorough EDA with much less effort. Furthermore, R/LinkedChart apps, typically first written as quick-and-dirty hacks, can also be polished to provide interactive data access in publication quality, thus contributing to open science.</p>
</div>
<div id="transparent-journalism-through-the-power-of-r" class="section level3">
<h3><span class="header-section-number">1.3.26</span> Transparent Journalism Through the Power of R</h3>
<p><em>Tatjana Kecojevic, SisterAnalyst.org; founder and director</em></p>
<p><strong>Track(s):</strong> R World</p>
<p><strong>Abstract:</strong></p>
<p>This study examines the often-tricky process of delivering data literacy programmes to professionals with most to gain from a deeper understanding of data analysis. As such, the author discusses the process of building and delivering training strategies to journalists in regions where press freedom is constrained by numerous factors, not least of all institutionalised corruption.</p>
<p>Reporting stories that are supplemented with transparent procedural systems are less likely to be contradicted and challenged by vested interest actors. Journalists are able to present findings supported by charts and info graphics, but these are open to translation. Therefore, most importantly, the data and code of the applied analytical methodology should also be available for scrutiny and is less likely to be subverted or prohibited.</p>
<p>As part of creating an accessible programme geared to acquiring skills necessary for data journalism, the author takes a step-by-step approach to discussing the actualities of building online platforms for training purposes. Through the use of grammar of graphics in R and Shiny, a web application framework for R, it is possible to develop interactive applications for graphical data visualisation. Presenting findings through interactive and accessible visualisation methods in a transparent and reproducible way is an effective form of reaching audiences that might not otherwise realise the value of the topic or data at hand.</p>
<p>The resulting ‘R toolbox for journalists’ is an accessible open-source resource. It can also be adapted to accommodate the need to provide a deeper understanding of the potential for data proficiency to other professions.</p>
<p>The accessibility of R allows for users to build support communities, which in the case of journalists is essential for information gathering. Establishing and implementing transparent channels of communication is the key to scrupulous journalism and is why R is so applicable to this objective.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="poster.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="shiny-demo.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
